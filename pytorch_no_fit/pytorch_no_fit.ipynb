{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"No fit?\" - Comparing high-level learning interfaces for PyTorch\n",
    "\n",
    "*Note: you can find the jupyter notebok for this post [here](https://github.com/sotte/sotte.github.io.resources).*\n",
    "\n",
    "I really like `PyTorch` and [I'm not alone](https://twitter.com/karpathy/status/868178954032513024).\n",
    "However, there is one aspect where `PyTorch` is not too user-friendly.\n",
    "`PyTorch` does not have a nice high level `fit` function,\n",
    "i.e. an interface like `scikit-learn`s or `keras` `fit`.\n",
    "That is the complaint I hear most often about `PyTorch`.\n",
    "\n",
    "I can't remember how often I have written a training loop in `PyTorch`\n",
    "and how often I made mistakes doing so.\n",
    "Writing a training loop is easy enough that anybody can do it,\n",
    "but tricky enough that everybody can get it subtly wrong when she/he isn't paying full attention.\n",
    "- Have you ever forgotten to call `model.eval()`? [[1]](https://twitter.com/karpathy/status/1013244313327681536)\n",
    "- Have you ever forgotten to zero the gradients? [[2]](https://twitter.com/karpathy/status/1013244313327681536)\n",
    "- Have you ever used the train data in the eval step?\n",
    "- Have you ever forgotten to move the data to the GPU?\n",
    "- Have you ever implemented a metric incorrectly?\n",
    "\n",
    "These problems would be void if `PyTorch` offered a `fit` function ala [keras](https://keras.io/) or [scikit-learn](https://scikit-learn.org/)\n",
    "(And, yes, you could argue that `PyTorch` is for power users and gives you all the power and flexibility so that you can implement the training loop tailored to your needs.\n",
    "However, even if there was a `fit` function you could still implement a custom training loop if you really had to.)\n",
    "\n",
    "In this post I'll evaluate the following high-level training libraries by solving a small image classification problem:\n",
    "- `ignite` https://github.com/pytorch/ignite\n",
    "- `skorch` https://github.com/dnouri/skorch\n",
    "- `PyToune` https://github.com/GRAAL-Research/pytoune\n",
    "- There are more ([tnt](https://github.com/pytorch/tnt), [fast.ai](https://github.com/fastai/fastai), ...) but it's too hot outside to spend more time in front of the computer.\n",
    "\n",
    "\n",
    "Note: This is a biased comparision!\n",
    "- I've used `ignite` before for small toy problems.\n",
    "- I looked at `skorch` but didn't use it because the support for `PyTorch` datasets seemed weird.\n",
    "- I've written my own little library and have my own ideas and preferances ;)\n",
    "\n",
    "## The Classification Problem\n",
    "I'll evaluate the three libraries by solving a simple image classification problem\n",
    "within a 30 minute timeframe.\n",
    "The demo task I'm trying to solve is a simple transfer learning task.\n",
    "The data is taken from the [Dogs vs Cats kaggle challenge](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "and wrapped in a `DogsAndCatsDataset` class.\n",
    "I'll use a pre-trained ResNet and only replace the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This is the usual setup for most ML tasks: data, model, loss, and optimizer.\n",
    "Feel free to skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet18\n",
    "\n",
    "import utils\n",
    "from utils import DogsCatsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and verified.\n",
      "Loading data from data/dogscats/train.\n",
      "Loading data from data/dogscats/valid.\n"
     ]
    }
   ],
   "source": [
    "def get_data(batch_size=64, sample=False):\n",
    "    IMG_SIZE = 224\n",
    "    _mean = [0.485, 0.456, 0.406]\n",
    "    _std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # transforms for dataset\n",
    "    train_trans = transforms.Compose([\n",
    "        # some images are too small to only crop --> resize first\n",
    "        transforms.Resize(256),\n",
    "        transforms.ColorJitter(.3, .3, .3),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(_mean, _std),\n",
    "    ])\n",
    "    val_trans = transforms.Compose([\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(_mean, _std),\n",
    "    ])\n",
    "\n",
    "    # dataset\n",
    "    train_ds = DogsCatsDataset(\n",
    "        \"data\",\n",
    "        \"sample/train\" if sample else \"train\",\n",
    "        transform=train_trans,\n",
    "        download=True,\n",
    "    )\n",
    "    val_ds = DogsCatsDataset(\n",
    "        \"data\",\n",
    "        \"sample/valid\" if sample else \"valid\",\n",
    "        transform=val_trans,\n",
    "    )\n",
    "\n",
    "    # data loader\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "train_dl, val_dl = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset DogsCatsDataset\n",
      "    Number of datapoints: 23000\n",
      "    Root Location: data/dogscats/train\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
      "                             ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0)\n",
      "                             CenterCrop(size=(224, 224))\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(train_dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Loss, and Optimizer\n",
    "We'll just use a simple pretrained ResNet and replace the last fully connected layer with a problem specific layer, i.e. a linear layer with two outputs (one for cats, one for dogs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = resnet18(pretrained=True)\n",
    "    utils.freeze_all(model.parameters())\n",
    "    assert utils.all_frozen(model.parameters())\n",
    "    \n",
    "    model.fc = nn.Linear(in_features=512, out_features=2)\n",
    "    assert utils.all_frozen(model.parameters()) is False\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify the loss function and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the sake of completeness, a *simple* train loop would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, optimizer, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} ...\")\n",
    "\n",
    "        # Train\n",
    "        model.train()  # IMPORTANT\n",
    "        running_loss, correct = 0.0, 0\n",
    "        for X, y in train_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_ = model(X)\n",
    "            loss = criterion(y_, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            #print(f\"    batch loss: {loss.item():0.3f}\")\n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "        print(\n",
    "            f\"  \"\n",
    "            f\"loss: {running_loss / len(train_dl.dataset):0.3f} \"\n",
    "            f\"acc:  {correct / len(train_dl.dataset):0.3f}\"\n",
    "        )\n",
    "\n",
    "        # Eval\n",
    "        model.eval()  # IMPORTANT\n",
    "        running_loss, correct = 0.0, 0\n",
    "        with torch.no_grad():  # IMPORTANT\n",
    "            for X, y in val_dl:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                y_ = model(X)\n",
    "                loss = criterion(y_, y)\n",
    "\n",
    "                _, y_label_ = torch.max(y_, 1)\n",
    "                correct += (y_label_ == y).sum().item()            \n",
    "                running_loss += loss.item() * X.shape[0]\n",
    "        print(\n",
    "            f\"  \"\n",
    "            f\"val_loss: {running_loss / len(val_dl.dataset):0.3f} \"\n",
    "            f\"val_acc:  {correct / len(val_dl.dataset):0.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 ...\n",
      "  loss: 0.322 acc:  0.886\n",
      "  val_loss: 0.204 val_acc:  0.938\n",
      "Epoch 2/2 ...\n",
      "  loss: 0.153 acc:  0.956\n",
      "  val_loss: 0.151 val_acc:  0.947\n",
      "CPU times: user 23 s, sys: 11.8 s, total: 34.8 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit(model, criterion, optimizer, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Contenders\n",
    "Here I will try to solve the task with the three libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignite\n",
    "> Ignite is a high-level library to help with training neural networks in PyTorch.\n",
    "> - ignite helps you write compact but full-featured training loops in a few lines of code\n",
    "> - you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n",
    "\n",
    "Github: https://github.com/pytorch/ignite\n",
    "\n",
    "Homepage: https://pytorch.org/ignite/\n",
    "\n",
    "`ignite` lives under the https://github.com/pytorch umbrella and can be installed with conda or pip:\n",
    "```\n",
    "conda install ignite -c pytorch\n",
    "pip install pytorch-ignite\n",
    "```\n",
    "\n",
    "`ignite` does not hide what's going on under the hood, but offers some light abstraction around the training loop.\n",
    "The main abstractions are `Engines` which loop over the data.\n",
    "The `State` object is part of the engine and is used to track training/evaluation state.\n",
    "Via `Events` and `Handlers` you can execute your custom code, e.g. printing out the current loss or storing a checkpoint.\n",
    "You can register callbacks via decorators:\n",
    "```python\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(trainer):\n",
    "    pass\n",
    "```\n",
    "\n",
    "`ignite` offers two helper functions, `create_supervised_trainer` and `create_supervised_evaluator`, which create `Engine`s for training and evaluating and should cover >90% or your supervised learning problems (I think).\n",
    "Even with these helpers, you still have to register callbacks to actually do something like logging and calculating of metrics (however, ignite offers some metrics).\n",
    "\n",
    "All in all I like the documentation. They have a `Quickstart` and a `Concepts` section which should get you going pretty quick.\n",
    "Out of the box, `ignite` does not give you any default logging or progress reports, but it's easy to add.\n",
    "However, I wish `ignite` offered this feature out of the box.\n",
    "\n",
    "I was done solving the task after ~20 minutes.\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and verified.\n",
      "Loading data from data/dogscats/train.\n",
      "Loading data from data/dogscats/valid.\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = get_data()\n",
    "\n",
    "model = get_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.handlers.timing.Timer at 0x7f6a832bfa90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The helper functions to create engines\n",
    "from ignite.engine import (\n",
    "    Events,\n",
    "    create_supervised_trainer,\n",
    "    create_supervised_evaluator,\n",
    ")\n",
    "# The metrics we're going to use\n",
    "from ignite.metrics import (\n",
    "    CategoricalAccuracy,\n",
    "    Loss,\n",
    ")\n",
    "from ignite.handlers import Timer\n",
    "\n",
    "# Setup\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "    },\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "# logging for output and metrics\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(trainer):\n",
    "    # too verbose\n",
    "    # print(\"Epoch[{}] Loss: {:.2f}\".format(trainer.state.epoch, trainer.state.output))\n",
    "    pass\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Training Results - Epoch: {trainer.state.epoch} \"\n",
    "        f\"Avg accuracy: {metrics['accuracy']:.2f} \"\n",
    "        f\"Avg loss: {metrics['loss']:.2f}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Validation Results - Epoch: {trainer.state.epoch} \"\n",
    "        f\"Avg accuracy: {metrics['accuracy']:.2f} \"\n",
    "        f\"Avg loss: {metrics['loss']:.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# let's measure the time\n",
    "timer = Timer(average=True)\n",
    "timer.attach(\n",
    "    trainer,\n",
    "    start=Events.EPOCH_STARTED,\n",
    "    resume=Events.ITERATION_STARTED,\n",
    "    pause=Events.ITERATION_STARTED,\n",
    "    step=Events.ITERATION_COMPLETED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch: 1 Avg accuracy: 0.96 Avg loss: 0.18\n",
      "Validation Results - Epoch: 1 Avg accuracy: 0.94 Avg loss: 0.21\n",
      "Training Results - Epoch: 2 Avg accuracy: 0.97 Avg loss: 0.12\n",
      "Validation Results - Epoch: 2 Avg accuracy: 0.96 Avg loss: 0.15\n",
      "CPU times: user 44.3 s, sys: 23.2 s, total: 1min 7s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f6ae424d908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.run(train_dl, max_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.506934350009033, 360.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.total, timer.step_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skorch\n",
    ">  A scikit-learn compatible neural network library that wraps pytorch.\n",
    ">\n",
    "> The goal of skorch is to make it possible to use PyTorch with sklearn. This is achieved by providing a wrapper around PyTorch that has an sklearn interface. In that sense, skorch is the spiritual successor to nolearn, but instead of using Lasagne and Theano, it uses PyTorch.\n",
    "\n",
    "Github: https://github.com/dnouri/skorch\n",
    "\n",
    "Homepage: https://skorch.readthedocs.io/en/latest/\n",
    "\n",
    "`skorch` is by the [Otto group](https://github.com/ottogroup/) and can be installed via pip\n",
    "```\n",
    "pip install skorch\n",
    "```\n",
    "The focus of `skorch` is to build a sklearn-like interface for PyTorch.\n",
    "I assume they use a lot of sklearn at Otto and they seamlessly want to intgrate PyTorch into their workflow (who could blame them).\n",
    "`skorch` also integrates into their serving service [palladium](https://github.com/ottogroup/palladium).\n",
    "\n",
    "`skorch` offers `NeuralNetClassifier` and `NeuralNetRegressor`.\n",
    "These classes wrap PyTorch's `nn.Module` and offer the sklearn-compatible interface\n",
    "(`fit`, `predict`, `predict_proba`, etc.).\n",
    "If you want more control you can create your own class and inherit from `skorch.NeuralNet`.\n",
    "Note that the `NeuralNet*` classes do internal cross validation.\n",
    "\n",
    "`skorch` reuses alot of the sklearn goodness (metrics, grid search, pipelines) and that's great.\n",
    "Additionally, `skorch` offers a simple Callback mechanism.\n",
    "The documentation is great and the library feels pretty complete.\n",
    "\n",
    "However, `skorch` does not allow me to wrap my existing datasets.\n",
    "Maybe it does but I was not able to find out how within 30 minutes.\n",
    "And I want to reuse all my Datasets :)\n",
    "\n",
    "> skorch uses the PyTorch DataLoaders by default. However, the Datasets provided by PyTorch are not sufficient for our usecase; for instance, they don’t work with numpy.ndarrays. \n",
    "\n",
    "Due to the \"dataset issue\" I was not able to finish the task within 30 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give you an idea of what the code looks like:\n",
    "```python\n",
    "from skorch.net import NeuralNetClassifier\n",
    "\n",
    "model = NeuralNetClassifier(model_, max_epochs=2, device=device)\n",
    "# I can't pass a dataloader\n",
    "# model.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyToune\n",
    "\n",
    "> PyToune is a Keras-like framework for PyTorch and handles much of the boilerplating code needed to train neural networks.\n",
    ">\n",
    "> Use PyToune to:\n",
    "> - Train models easily.\n",
    "> - Use callbacks to save your best model, perform early stopping and much more.\n",
    "\n",
    "Github: https://github.com/GRAAL-Research/pytoune\n",
    "\n",
    "Homepage: https://pytoune.org/\n",
    "\n",
    "`PyToune` is a relatively young project by [GRAAL-Research](https://github.com/GRAAL-Research).\n",
    "It can be installed with pip:\n",
    "```bash\n",
    "pip install pytoune\n",
    "```\n",
    "\n",
    "`PyToune` feels very keras-y and I had a working version with progress reports and whatnot after just 7 minutes.\n",
    "The main abstraction is the `Model` which takes a `PyTorch` `nn.Module`, an optimizer, and a loss function.\n",
    "The `Model` then gives you an interface that is very similar to keras\n",
    "(`fit()`, `fit_generator()`, `evaluate_generator()`, etc).\n",
    "Additionally, you have a generic callback mechanism to interact with the opitmization process.\n",
    "There are also some useful callbacks implemented (`ModelCheckpoint`, `EarlyStopping`, `TerminateOnNaN`, `BestModelRestore`, and wrappers for `PyTorch`'s learning rate schedulers).\n",
    "`PyToune` offers some convenient layers like `Flatten`, `Identity`, and `Lambda` (I'm sure we've all written these many times, so I appreciate that :)).\n",
    "\n",
    "The documentation is very short (just api docs, but good ones) and could use a \"getting started\" guide and  more narrative docs.\n",
    "However, `PyToune` is so simple (in the best sense possible) that you are productive within a few minutes.\n",
    "\n",
    "All in all: nice! The docs should be extended and I wish there were more metrics, but `PyToune` looks great.\n",
    "I'm planning to use/evaluate it with some of my projects.\n",
    "\n",
    "After 25 minutes I stopped because there wasn't anything to do anymore :)\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and verified.\n",
      "Loading data from data/dogscats/train.\n",
      "Loading data from data/dogscats/valid.\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = get_data()\n",
    "\n",
    "model_ = get_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytoune.framework import Model\n",
    "from pytoune.framework.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Model(model_, optimizer, criterion, metrics=[\"accuracy\"])\n",
    "model = model.to(device)\n",
    "\n",
    "model_checkpoint_cb = ModelCheckpoint(\n",
    "    \"pytoune_experiment_best_epoch_{epoch}.ckpt\",\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    restore_best=True,\n",
    "    verbose=False,\n",
    "    temporary_filename=\"best_epoch.ckpt.tmp\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 29.92s Step 360/360: loss: 0.331596, acc: 88.656522, val_loss: 0.202725, val_acc: 94.450000\n",
      "Epoch 2/2 27.26s Step 360/360: loss: 0.153364, acc: 95.800000, val_loss: 0.147647, val_acc: 95.500000\n",
      "CPU times: user 24.9 s, sys: 13.2 s, total: 38.2 s\n",
      "Wall time: 57.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'loss': 0.3315962664977364,\n",
       "  'acc': 88.65652173913044,\n",
       "  'val_loss': 0.20272484612464906,\n",
       "  'val_acc': 94.45},\n",
       " {'epoch': 2,\n",
       "  'loss': 0.15336425514584,\n",
       "  'acc': 95.8,\n",
       "  'val_loss': 0.14764661401510237,\n",
       "  'val_acc': 95.5}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(\n",
    "    train_dl,\n",
    "    valid_generator=val_dl,\n",
    "    callbacks=[model_checkpoint_cb],\n",
    "    epochs=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytoune_experiment_best_epoch_1.ckpt  pytoune_experiment_best_epoch_2.ckpt\n"
     ]
    }
   ],
   "source": [
    "ls pytoune_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All libraries look good.\n",
    "\n",
    "- `ignite` is a elegant wrapper around `PyTorch`. However, it's a bit too low level for what I'm looking for.\n",
    "\n",
    "- `skorch` is very complete and offers a ton of features. Sadly it does not play well with Dataset classes.\n",
    "\n",
    "- `PyToune` clicked right away and if you're familiar with `keras` I'm sure it will click for you as well. \n",
    "\n",
    "I highly encourage you to check out `PyToune`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
